{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2416cb70-ea9a-40ff-a407-536fa254fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "import igraph as ig\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Callable, Union\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f27d58d-a124-46b1-b10e-e342b17f869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def set_random_seed(seed):\n",
    "    #for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def is_dag(W):\n",
    "    #checks with weight adjancency matrix is a DAG\n",
    "    G = ig.Graph.Weighted_Adjacency(W.tolist())\n",
    "    return G.is_dag()\n",
    "\n",
    "def simulate_dag(d, s0, graph_type):\n",
    "    \"\"\"simulates random DAG with some expected number of edges\n",
    "    Args:\n",
    "        d (int): num of nodes\n",
    "        s0 (int): expected num of edges\n",
    "        graph_type (str): ER, SF, BP\n",
    "    Returns:\n",
    "        B (np.ndarray): [d, d] binary adj matrix of DAG\n",
    "    \"\"\"\n",
    "    def random_permutation(M):\n",
    "        #permutes rows and columns of a matrix\n",
    "        P = np.random.permutation(np.eye(M.shape[0]))\n",
    "        return P.T @ M @ P\n",
    "\n",
    "    def random_acyclic_orientation(B_und):\n",
    "        #takes undirected graph matrix and returns a DAG\n",
    "        return np.tril(random_permutation(B_und), k=-1)\n",
    "\n",
    "    def graph_adjmat(G):\n",
    "        #converts igraph graph object to numpy adjacency matrix\n",
    "        return np.array(G.get_adjacency().data)\n",
    "\n",
    "    if graph_type == 'ER':\n",
    "        # Erdos-Renyi: set probability for an edge to be inserted \n",
    "        G_und = ig.Graph.Erdos_Renyi(n=d, m=s0) # undirected graph with d nodes and s0 edges\n",
    "        B_und = graph_adjmat(G_und)\n",
    "        B = random_acyclic_orientation(B_und) #randomly set orientation for acyclic\n",
    "    elif graph_type == 'SF':\n",
    "        # Scale-free, Barabasi-Albert: certain nodes have more edges (hubs)\n",
    "        G = ig.Graph.Barabasi(n=d, m=int(round(s0 / d)), directed=True)\n",
    "        B = graph_adjmat(G)\n",
    "    elif graph_type == 'BP':\n",
    "        # Bipartite: two sets of nodes with edges only between the two sets\n",
    "        top = int(0.2 * d)\n",
    "        G = ig.Graph.Random_Bipartite(top, d - top, m=s0, directed=True, neimode=ig.OUT)\n",
    "        B = graph_adjmat(G)\n",
    "    else:\n",
    "        raise ValueError('unknown graph type')\n",
    "\n",
    "    #another permuation to randomize order\n",
    "    B_perm = random_permutation(B)\n",
    "    assert ig.Graph.Adjacency(B_perm.tolist()).is_dag()\n",
    "    return B_perm\n",
    "\n",
    "def simulate_parameter(B, w_ranges=((-2.0, -0.5), (0.5, 2.0))):\n",
    "    \"\"\"simulate Structural Equation Model (SEM) parameters for DAG\n",
    "    Args:\n",
    "        B (np.ndarray): [d, d] binary adj matrix of DAG\n",
    "        w_ranges (tuple): disjoint weight ranges\n",
    "    Returns:\n",
    "        W (np.ndarray): [d, d] weighted adj matrix of DAG\n",
    "    \"\"\"\n",
    "    W = np.zeros(B.shape)\n",
    "    #randomly select which range of weights each edge is\n",
    "    S = np.random.randint(len(w_ranges), size=B.shape)  # which range\n",
    "    for i, (low, high) in enumerate(w_ranges):\n",
    "        #generate uniform random weights\n",
    "        U = np.random.uniform(low=low, high=high, size=B.shape)\n",
    "        W += B * (S == i) * U\n",
    "    return W\n",
    "\n",
    "def simulate_linear_sem(W, n, sem_type, noise_scale=None, discrete_ratio=0.0, max_categories=10):\n",
    "    \"\"\"\n",
    "    simulate samples from linear SEM with specified type of noise and mixed continuous/discrete variables\n",
    "    Args:\n",
    "        W (np.ndarray): weighted adjacency matrix\n",
    "        n (int): # of samples to generate\n",
    "        sem_type (str): Type of noise/non-linearity ('gaussian', 'uniform', 'poisson', etc.)\n",
    "        noise_scale (float/np.ndarray): Scale parameter for the noise\n",
    "        discrete_ratio (float): Proportion of nodes that should be discrete\n",
    "        max_categories (int): Max number of categories for discrete nodes\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def simulate_single_equation(X, w, scale, is_discrete=False, n_cats=None):\n",
    "        \"\"\"\n",
    "        X: [n, num of parents]\n",
    "        w: [num of parents], x: [n]\n",
    "        \"\"\"\n",
    "        if is_discrete: #discrete case: uses a multinomial logistic model\n",
    "            if X.shape[1] == 0: #no parents: noise directly defines logits\n",
    "                logits = np.random.normal(scale=scale, size=(n, n_cats))\n",
    "            else:\n",
    "                #calculate logits based on a linear combination of parent values\n",
    "                logits = np.zeros((n, n_cats))\n",
    "                for k in range(n_cats):\n",
    "                    #linear componenent for each category k\n",
    "                    if sem_type == 'gaussian':\n",
    "                        logits[:, k] = X @ w\n",
    "                    elif sem_type == 'exponential':\n",
    "                        logits[:, k] = X @ w\n",
    "                    elif sem_type == 'gumbel':\n",
    "                        logits[:, k] = X @ w\n",
    "                    elif sem_type == 'uniform':\n",
    "                        logits[:, k] = X @ w\n",
    "                    else:\n",
    "                        raise ValueError('unsupported sem type for discrete variables')\n",
    "            # logit -> probability\n",
    "            probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "            #sample categories based on probabilities\n",
    "            return np.array([np.random.choice(n_cats, p=p) for p in probs]).astype(float)\n",
    "        else:\n",
    "            if sem_type == 'gaussian':\n",
    "                z = np.random.normal(scale=scale, size=n)\n",
    "                x = X @ w + z\n",
    "            elif sem_type == 'exponential':\n",
    "                z = np.random.exponential(scale=scale, size=n)\n",
    "                x = X @ w + z\n",
    "            elif sem_type == 'gumbel':\n",
    "                z = np.random.gumbel(scale=scale, size=n)\n",
    "                x = X @ w + z\n",
    "            elif sem_type == 'uniform':\n",
    "                z = np.random.uniform(low=-scale, high=scale, size=n)\n",
    "                x = X @ w + z\n",
    "            elif sem_type == 'logistic':\n",
    "                x = np.random.binomial(1, sigmoid(X @ w)) * 1.0\n",
    "            elif sem_type == 'poisson':\n",
    "                x = np.random.poisson(np.exp(X @ w)) * 1.0\n",
    "            else:\n",
    "                raise ValueError('unknown sem type')\n",
    "            return x\n",
    "\n",
    "    d = W.shape[0]\n",
    "\n",
    "    #noise scale\n",
    "    if noise_scale is None:\n",
    "        scale_vec = np.ones(d)\n",
    "    elif np.isscalar(noise_scale):\n",
    "        scale_vec = noise_scale * np.ones(d)\n",
    "    else:\n",
    "        if len(noise_scale) != d:\n",
    "            raise ValueError('noise scale must be a scalar or has length d')\n",
    "        scale_vec = noise_scale\n",
    "    if not is_dag(W):\n",
    "        raise ValueError('W must be a DAG')\n",
    "    if np.isinf(n):\n",
    "        if sem_type == 'gauss':\n",
    "            X = np.sqrt(d) * np.diag(scale_vec) @ np.linalg.inv(np.eye(d) - W)\n",
    "            return X\n",
    "        else:\n",
    "            raise ValueError('population risk not available')\n",
    "\n",
    "\n",
    "    #setting up which columns will be discrete variables\n",
    "    n_discrete = int(d * discrete_ratio)\n",
    "    discrete_cols = np.random.choice(d, size=n_discrete, replace=False)\n",
    "\n",
    "    #topological order for simulation\n",
    "    G = ig.Graph.Weighted_Adjacency(W.tolist())\n",
    "    ordered_vertices = G.topological_sorting()\n",
    "    \n",
    "    assert len(ordered_vertices) == d\n",
    "    X = np.zeros([n, d]) #data matrix\n",
    "    for j in ordered_vertices:\n",
    "        #loop through nodes in causal order\n",
    "        parents = G.neighbors(j, mode=ig.IN)\n",
    "        is_discrete = j in discrete_cols\n",
    "        \n",
    "        #determining how many categories for discrete variables\n",
    "        if max_categories > 2:\n",
    "            n_categories = np.random.randint(2, max_categories)\n",
    "        else:\n",
    "            n_categories = 2\n",
    "\n",
    "        #getting X[:, j] from X[:, parents]\n",
    "        X[:, j] = simulate_single_equation(X[:, parents], W[parents, j], scale_vec[j], is_discrete, n_categories)\n",
    "    return X\n",
    "\n",
    "def simulate_nonlinear_sem(B, n, sem_type, noise_scale=None, discrete_ratio=0.3, max_categories=10):\n",
    "    \"\"\"simulate samples from nonlinear SEM with mixed continuous and discrete variables.\n",
    "    Args:\n",
    "        B (np.ndarray): Binary adjacency matrix\n",
    "        n (int): # of samples to generate\n",
    "        sem_type (str): Type of non-linear function ('mlp', 'mim', 'gp', 'gp-add')\n",
    "        noise_scale (float): Scale parameter for the noise (assumed Gaussian)\n",
    "        discrete_ratio (float): Proportion of nodes that should be discrete\n",
    "        max_categories (int): Max # of categories for discrete nodes\n",
    "    \"\"\"\n",
    "    def simulate_single_equation(X, scale, is_discrete=False, n_cats=None):\n",
    "        #simulates value of a single variable using nonlinear function\n",
    "        pa_size = X.shape[1]\n",
    "        \n",
    "        if is_discrete:\n",
    "            if pa_size == 0:\n",
    "                logits = np.random.normal(scale=scale, size=(n, n_cats))\n",
    "            else:\n",
    "                if sem_type == 'mlp': #Multi-Layer Perceptron\n",
    "                    hidden = 100\n",
    "                    logits = np.zeros((n, n_cats))\n",
    "                    for k in range(n_cats): #randomly assign nonlinear weights to each category\n",
    "                        W1 = np.random.uniform(low=0.5, high=2.0, size=[pa_size, hidden])\n",
    "                        W1[np.random.rand(*W1.shape) < 0.5] *= -1\n",
    "                        W2 = np.random.uniform(low=0.5, high=2.0, size=hidden)\n",
    "                        W2[np.random.rand(hidden) < 0.5] *= -1\n",
    "                        logits[:, k] = sigmoid(X @ W1) @ W2\n",
    "                elif sem_type == 'mim': #combo of sin, cos, tan\n",
    "                    logits = np.zeros((n, n_cats))\n",
    "                    #randomly assign linear combination of weights\n",
    "                    for k in range(n_cats):\n",
    "                        w1 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                        w1[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                        w2 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                        w2[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                        w3 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                        w3[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                        logits[:, k] = np.tanh(X @ w1) + np.cos(X @ w2) + np.sin(X @ w3)\n",
    "                elif sem_type == 'gp' or sem_type == 'gp-add':\n",
    "                    logits = np.zeros((n, n_cats))\n",
    "                    for k in range(n_cats):\n",
    "                        gp = GaussianProcessRegressor()\n",
    "                        if sem_type == 'gp':\n",
    "                            logits[:, k] = gp.sample_y(X, random_state=None).flatten()\n",
    "                        else:  # gp-add\n",
    "                            logits[:, k] = sum([gp.sample_y(X[:, i, None], random_state=None).flatten() \n",
    "                                                for i in range(X.shape[1])])\n",
    "                else:\n",
    "                    raise ValueError('unknown sem type')\n",
    "            \n",
    "            probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "            return np.array([np.random.choice(n_cats, p=p) for p in probs]).astype(float)\n",
    "            \n",
    "        else:\n",
    "            z = np.random.normal(scale=scale, size=n)\n",
    "            if pa_size == 0:\n",
    "                return z\n",
    "                \n",
    "            if sem_type == 'mlp':\n",
    "                hidden = 100\n",
    "                W1 = np.random.uniform(low=0.5, high=2.0, size=[pa_size, hidden])\n",
    "                W1[np.random.rand(*W1.shape) < 0.5] *= -1\n",
    "                W2 = np.random.uniform(low=0.5, high=2.0, size=hidden)\n",
    "                W2[np.random.rand(hidden) < 0.5] *= -1\n",
    "                x = sigmoid(X @ W1) @ W2 + z\n",
    "            elif sem_type == 'mim':\n",
    "                w1 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                w1[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                w2 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                w2[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                w3 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                w3[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                x = np.tanh(X @ w1) + np.cos(X @ w2) + np.sin(X @ w3) + z\n",
    "            elif sem_type == 'gp':\n",
    "                gp = GaussianProcessRegressor()\n",
    "                x = gp.sample_y(X, random_state=None).flatten() + z\n",
    "            elif sem_type == 'gp-add':\n",
    "                gp = GaussianProcessRegressor()\n",
    "                x = sum([gp.sample_y(X[:, i, None], random_state=None).flatten()\n",
    "                         for i in range(X.shape[1])]) + z\n",
    "            else:\n",
    "                raise ValueError('unknown sem type')\n",
    "            return x\n",
    "\n",
    "    d = B.shape[0]\n",
    "    scale_vec = noise_scale * np.ones(d) if noise_scale else np.ones(d)\n",
    "    X = np.zeros([n, d])\n",
    "    G = ig.Graph.Adjacency(B.tolist())\n",
    "    ordered_vertices = G.topological_sorting()\n",
    "    assert len(ordered_vertices) == d\n",
    "    \n",
    "    n_discrete = int(d * discrete_ratio)\n",
    "    discrete_columns = np.random.choice(d, size=n_discrete, replace=False)\n",
    "    \n",
    "    for j in ordered_vertices:\n",
    "        parents = G.neighbors(j, mode=ig.IN)\n",
    "        is_discrete = j in discrete_columns\n",
    "        \n",
    "        # CHANGE: checker\n",
    "        # would fail if max_categories was 2 (since low >= high)\n",
    "        if max_categories > 2:\n",
    "            n_categories = np.random.randint(2, max_categories)\n",
    "        else:\n",
    "            n_categories = 2\n",
    "\n",
    "        X[:, j] = simulate_single_equation(X[:, parents], scale_vec[j], is_discrete, n_categories)\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821748b1-19ca-4b1d-b34f-5ca68fe8044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSimulator Class from repo (modified) \n",
    "\n",
    "class DataSimulator:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.graph = None\n",
    "        self.ground_truth = {}\n",
    "        self.variable_names = None\n",
    "\n",
    "    def generate_graph(self, n_nodes: int, edge_probability: float = 0.3, variable_names: List[str] = None, graph_type: str = 'ER') -> None:\n",
    "        \"\"\"Generate a random directed acyclic graph (DAG) using specified method.\"\"\"\n",
    "        self.graph = simulate_dag(n_nodes, int(edge_probability * n_nodes * (n_nodes - 1) / 2), graph_type)\n",
    "        if variable_names and len(variable_names) == n_nodes:\n",
    "            self.variable_names = variable_names\n",
    "            self.graph_dict = {i: name for i, name in enumerate(variable_names)}\n",
    "        else:\n",
    "            self.variable_names = [f'X{i+1}' for i in range(n_nodes)]\n",
    "            self.graph_dict = {i: f'X{i+1}' for i in range(n_nodes)}\n",
    "        self.ground_truth['graph'] = self.graph_dict\n",
    "        self.ground_truth['edge_probability'] = edge_probability\n",
    "\n",
    "    def generate_single_domain_data(self, n_samples: int, noise_scale: float, noise_type: str, \n",
    "                                      function_type: Union[str, List[str], Dict[str, str]], \n",
    "                                      discrete_ratio: float = 0.0, max_categories: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Generate data for a single domain based on the graph structure.\"\"\"\n",
    "        # CHANGED: Replaced 'logger.debug' with print\n",
    "        print(f\"Data generation: {function_type}, {noise_type}, scale={noise_scale}\")\n",
    "        assert isinstance(function_type, str) and function_type in ['linear', 'mlp', 'mim', 'gp', 'gp-add']\n",
    "        assert noise_type in ['gaussian', 'exponential', 'gumbel', 'uniform', 'logistic', 'poisson']\n",
    "        if function_type != 'linear':\n",
    "            # CHANGED: replaced logger.debug with print\n",
    "            print(\"Non-linear function requires gaussian noise\")\n",
    "        assert isinstance(noise_scale, float) and noise_scale > 0\n",
    "        if function_type == 'linear':\n",
    "            W = simulate_parameter(self.graph)\n",
    "            data = simulate_linear_sem(W, n_samples, noise_type, noise_scale, discrete_ratio, max_categories)\n",
    "        else:\n",
    "            data = simulate_nonlinear_sem(self.graph, n_samples, function_type, noise_scale, discrete_ratio, max_categories)\n",
    "        \n",
    "        data_df = pd.DataFrame(data, columns=self.variable_names)\n",
    "        return data_df\n",
    "    \n",
    "    def generate_multi_domain_data(self, n_samples: int, noise_scale: float, noise_type: str, \n",
    "                                     function_type: Union[str, List[str], Dict[str, str]], \n",
    "                                     discrete_ratio: float = 0.0, max_categories: int = 5, edge_probability: float = 0.3) -> pd.DataFrame:\n",
    "        \"\"\"Generate data for multiple domains based on the graph structure.\"\"\"\n",
    "        # CHANGED: Replaced 'logger.debug' with print\n",
    "        print(f\"Data generation: {function_type}, {noise_type}, scale={noise_scale}\")\n",
    "        assert isinstance(function_type, str) and function_type in ['linear', 'mlp', 'mim', 'gp', 'gp-add']\n",
    "        assert noise_type in ['gaussian', 'exponential', 'gumbel', 'uniform', 'logistic', 'poisson']\n",
    "        if function_type != 'linear':\n",
    "            # CHANGED: replaced logger.debug with print\n",
    "            print(\"Non-linear function requires gaussian noise\")\n",
    "        assert isinstance(noise_scale, float) and noise_scale > 0\n",
    "\n",
    "        if function_type == 'linear':\n",
    "            W = simulate_parameter(self.graph)\n",
    "            base_data = []\n",
    "            for i in range(self.n_domains):\n",
    "                base_data.append(simulate_linear_sem(W, n_samples, noise_type, noise_scale, discrete_ratio, max_categories))\n",
    "        else:\n",
    "            base_data = []\n",
    "            for i in range(self.n_domains):\n",
    "                base_data.append(simulate_nonlinear_sem(self.graph, n_samples, function_type, noise_scale, discrete_ratio, max_categories))\n",
    "        \n",
    "        n_nodes = self.graph.shape[0]\n",
    "        node_connections = np.sum(self.graph, axis=1) + np.sum(self.graph, axis=0)\n",
    "        less_connected_nodes = np.argsort(node_connections)\n",
    "        n_affected = max(3, int(edge_probability * n_nodes))\n",
    "        affected_nodes = less_connected_nodes[:n_affected]\n",
    "        \n",
    "        # CHANGED: replaced logger.debug with print\n",
    "        print(f\"Domain affects {len(affected_nodes)} variables\")\n",
    "        \n",
    "        data = []\n",
    "        step = 10 / self.n_domains\n",
    "        \n",
    "        base_data_array = np.vstack([d for d in base_data])\n",
    "        base_correlation_matrix = np.corrcoef(base_data_array.T)\n",
    "        # CHANGED: replaced logger.debug with print\n",
    "        print(f\"Base correlation matrix: {base_correlation_matrix.shape}\")\n",
    "        \n",
    "        n_vars = base_correlation_matrix.shape[0]\n",
    "        high_corr_threshold = 0.7\n",
    "        high_corr_pairs_base = []\n",
    "        \n",
    "        for i in range(n_vars):\n",
    "            for j in range(i+1, n_vars):\n",
    "                corr = base_correlation_matrix[i, j]\n",
    "                if abs(corr) > high_corr_threshold:\n",
    "                    high_corr_pairs_base.append((self.variable_names[i], self.variable_names[j], corr))\n",
    "        \n",
    "        if high_corr_pairs_base:\n",
    "            # CHANGED: replaced logger.debug with print\n",
    "            print(f\"Found {len(high_corr_pairs_base)} highly correlated pairs (base)\")\n",
    "        else:\n",
    "            # CHANGED: replaced logger.debug with print\n",
    "            print(f\"No high correlations found (base)\")\n",
    "        \n",
    "        domain_effect_multiplier = 0.3\n",
    "        max_attempts = 5\n",
    "        attempts = 0\n",
    "        new_high_corr_pairs_found = False\n",
    "        \n",
    "        temp_data = [] # Initialize here\n",
    "        \n",
    "        while attempts < max_attempts and not new_high_corr_pairs_found:\n",
    "            temp_data = []\n",
    "            for i in range(self.n_domains):\n",
    "                domain_data = base_data[i].copy()\n",
    "                \n",
    "                if function_type == 'linear':\n",
    "                    domain_effect = (i + 1) * step * domain_effect_multiplier\n",
    "                    for node_idx in affected_nodes:\n",
    "                        domain_data[:, node_idx] += domain_effect\n",
    "                else:\n",
    "                    domain_effect = (i + 1) * step * domain_effect_multiplier\n",
    "                    for node_idx in affected_nodes:\n",
    "                        base_values = domain_data[:, node_idx]\n",
    "                        domain_data[:, node_idx] += domain_effect * np.sign(base_values) * (base_values ** 2)\n",
    "                \n",
    "                temp_data.extend(domain_data)\n",
    "            \n",
    "            temp_data_array = np.vstack(temp_data)\n",
    "            temp_correlation_matrix = np.corrcoef(temp_data_array.T)\n",
    "            \n",
    "            high_corr_pairs_temp = []\n",
    "            for i in range(n_vars):\n",
    "                for j in range(i+1, n_vars):\n",
    "                    corr = temp_correlation_matrix[i, j]\n",
    "                    if abs(corr) > high_corr_threshold:\n",
    "                        high_corr_pairs_temp.append((self.variable_names[i], self.variable_names[j], corr))\n",
    "            \n",
    "            if len(high_corr_pairs_temp) > len(high_corr_pairs_base):\n",
    "                new_high_corr_pairs_found = True\n",
    "                data = temp_data\n",
    "                correlation_matrix = temp_correlation_matrix\n",
    "                high_corr_pairs = high_corr_pairs_temp\n",
    "            else:\n",
    "                domain_effect_multiplier *= 1.5\n",
    "                attempts += 1\n",
    "        \n",
    "        if not new_high_corr_pairs_found:\n",
    "            data = temp_data\n",
    "            correlation_matrix = temp_correlation_matrix\n",
    "            high_corr_pairs = high_corr_pairs_temp\n",
    "        \n",
    "        #  CHANGED: replaced logger.debug with print\n",
    "        print(f\"Final correlation matrix: {correlation_matrix.shape}\")\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            #  CHANGED: replaced logger.debug with print\n",
    "            print(f\"Found {len(high_corr_pairs)} highly correlated pairs (final)\")\n",
    "        else:\n",
    "            #  CHANGED: replaced logger.debug with print\n",
    "            print(f\"No high correlations found (final)\")\n",
    "            \n",
    "        data_df = pd.DataFrame(data, columns=self.variable_names)\n",
    "        data_df['domain_index'] = np.repeat(range(1, 1 + self.n_domains), n_samples)\n",
    "        return data_df\n",
    "\n",
    "    def generate_data(self, n_samples: int, noise_scale: float = 1.0, \n",
    "                        noise_type: str = 'gaussian', \n",
    "                        function_type: Union[str, List[str], Dict[str, str]] = 'linear',\n",
    "                        n_domains: int = 1, variable_names: List[str] = None,\n",
    "                        discrete_ratio: float = 0.0, max_categories: int = 5, edge_probability: float = 0.3) -> None:\n",
    "        \"\"\"Generate heterogeneous data from multiple domains.\"\"\"\n",
    "        if self.graph is None:\n",
    "            raise ValueError(\"Generate graph first\")\n",
    "\n",
    "        domain_size = n_samples // n_domains\n",
    "        self.n_domains = n_domains\n",
    "\n",
    "        if n_domains == 1:\n",
    "            domain_df = self.generate_single_domain_data(domain_size, noise_scale, noise_type, function_type, \n",
    "                                                         discrete_ratio, max_categories)\n",
    "        else:\n",
    "            domain_df = self.generate_multi_domain_data(domain_size, noise_scale, noise_type, function_type, \n",
    "                                                        discrete_ratio, max_categories, edge_probability)\n",
    "        \n",
    "        self.data = domain_df\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        if variable_names is not None:\n",
    "            if n_domains == 1:\n",
    "                self.data.columns = variable_names\n",
    "            else:\n",
    "                self.data.columns = variable_names + ['domain_index']\n",
    "                \n",
    "        self.ground_truth['n_domains'] = n_domains\n",
    "        self.ground_truth['noise_type'] = noise_type\n",
    "        self.ground_truth['function_type'] = function_type\n",
    "        self.ground_truth['discrete_ratio'] = discrete_ratio\n",
    "        self.ground_truth['max_categories'] = max_categories\n",
    "\n",
    "    def add_measurement_error(self, error_std: float = 0.3, error_rate: float = 0.5) -> None:\n",
    "        \"\"\"Randomly sample a subset of columns to add gaussian measurement error.\"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Generate data first\")\n",
    "\n",
    "        available_cols = [col for col in self.data.columns if col != 'domain_index']\n",
    "        n_cols = int(error_rate * len(available_cols))\n",
    "        columns = np.random.choice(available_cols, size=n_cols, replace=False)\n",
    "\n",
    "        for col in columns:\n",
    "            if pd.api.types.is_numeric_dtype(self.data[col]):\n",
    "                self.data[col] += np.random.randn(len(self.data)) * error_std\n",
    "        \n",
    "        self.ground_truth['measurement_error'] = {col: error_std for col in columns}\n",
    "        self.ground_truth['measurement_error_value'] = error_rate\n",
    "        self.ground_truth['measurement_error_desc'] = f\"The measurement error is Gaussian with a standard deviation of {error_std} on {error_rate} of the columns (some are selected, some are not, except domain_index), and the measurement error is added to the original data.\"\n",
    "\n",
    "    def add_missing_values(self, missing_rate: float = 0.1) -> None:\n",
    "        \"\"\"Introduce missing values to the whole dataframe with a specified missing rate.\"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Generate data first\")\n",
    "\n",
    "        columns = [col for col in self.data.columns if col != 'domain_index']\n",
    "        \n",
    "        mask = np.random.random(size=self.data[columns].shape) < missing_rate\n",
    "        \n",
    "        self.data[columns] = np.where(mask, np.nan, self.data[columns])\n",
    "        \n",
    "        affected_columns = [col for col in columns if mask[:, columns.index(col)].any()]\n",
    "        self.ground_truth['missing_rate'] = {col: missing_rate for col in affected_columns}\n",
    "        self.ground_truth['missing_rate_value'] = missing_rate\n",
    "        self.ground_truth['missing_data_desc'] = f\"The missing values are randomly sampled with a missing rate of {missing_rate} on all column data (except domain_index), and the missing values are replaced with NaN.\"\n",
    "\n",
    "    def generate_dataset(self, n_nodes: int, n_samples: int, edge_probability: float = 0.3,\n",
    "                         noise_scale: float = 1.0, noise_type: str = 'gaussian',\n",
    "                         function_type: Union[str, List[str], Dict[str, str]] = 'linear', discrete_ratio: float = 0.0, max_categories: int = 5,\n",
    "                         add_measurement_error: bool = False, add_missing_values: bool = False, n_domains: int = 1, \n",
    "                         error_std: float = 0.3, error_rate: float = 0.5, missing_rate: float = 0.1,\n",
    "                         variable_names: List[str] = None, graph_type: str = 'ER') -> Tuple[Dict[int, str], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Generate a complete heterogeneous dataset with various characteristics.\n",
    "        \"\"\"\n",
    "        self.generate_graph(n_nodes, edge_probability, variable_names, graph_type)\n",
    "        self.generate_data(n_samples, noise_scale, noise_type, function_type, n_domains, variable_names, discrete_ratio, max_categories, edge_probability)\n",
    "                \n",
    "        if add_measurement_error:\n",
    "            self.add_measurement_error(error_std=error_std, error_rate=error_rate)\n",
    "        else:\n",
    "            self.ground_truth['measurement_error'] = None\n",
    "            self.ground_truth['measurement_error_value'] = None\n",
    "            self.ground_truth['measurement_error_desc'] = None\n",
    "        \n",
    "        if add_missing_values:\n",
    "            self.add_missing_values(missing_rate=missing_rate)\n",
    "        else:\n",
    "            self.ground_truth['missing_rate'] = None\n",
    "            self.ground_truth['missing_rate_value'] = None\n",
    "            self.ground_truth['missing_data_desc'] = None\n",
    "        \n",
    "        return self.graph.T, self.data\n",
    "\n",
    "    def save_simulation(self, output_dir: str = 'simulated_data', prefix: str = 'base') -> None:\n",
    "        \"\"\"\n",
    "        Save the simulated data, graph structure, and simulation settings.\n",
    "        \"\"\"\n",
    "        if self.data is None or self.graph is None:\n",
    "            raise ValueError(\"No data or graph to save. Generate dataset first.\")\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        n_nodes = len(self.graph)\n",
    "        n_samples = len(self.data)\n",
    "        folder_name = f\"{timestamp}_{prefix}_nodes{n_nodes}_samples{n_samples}\"\n",
    "        save_dir = os.path.join(output_dir, folder_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        data_filename = os.path.join(save_dir, f'{prefix}_data.csv')\n",
    "        data_to_save = self.data.copy()\n",
    "        if self.ground_truth.get('n_domains', 1) == 1 and 'domain_index' in data_to_save.columns:\n",
    "            data_to_save = data_to_save.drop('domain_index', axis=1)\n",
    "        data_to_save.to_csv(data_filename, index=False)\n",
    "        # <<< CHANGE: Replaced 'logger.success' with 'print', keeping your notebook's success format.\n",
    "        print(f\"\\u001b[92m✓ SUCCESS\\u001b[0m Data saved: {data_filename}\")\n",
    "        \n",
    "        graph_filename = os.path.join(save_dir, f'{prefix}_graph.npy')\n",
    "        np.save(graph_filename, self.graph.T)\n",
    "        # <<< CHANGE: Replaced 'logger.success' with 'print'.\n",
    "        print(f\"\\u001b[92m✓ SUCCESS\\u001b[0m Graph saved: {graph_filename}\")\n",
    "        \n",
    "        config_filename = os.path.join(save_dir, f'{prefix}_config.json')\n",
    "        config = {\n",
    "            'n_nodes': n_nodes,\n",
    "            'n_samples': n_samples,\n",
    "            'n_domains': self.ground_truth.get('n_domains'),\n",
    "            'noise_type': self.ground_truth.get('noise_type'),\n",
    "            'function_type': self.ground_truth.get('function_type'),\n",
    "            'node_functions': self.ground_truth.get('node_functions'),\n",
    "            'categorical': self.ground_truth.get('categorical'),\n",
    "            'measurement_error': self.ground_truth.get('measurement_error'),\n",
    "            'selection_bias': self.ground_truth.get('selection_bias'),\n",
    "            'confounding': self.ground_truth.get('confounding'),\n",
    "            'missing_rate': self.ground_truth.get('missing_rate'),\n",
    "            'edge_probability': self.ground_truth.get('edge_probability'),\n",
    "            'discrete_ratio': self.ground_truth.get('discrete_ratio'),\n",
    "            'max_categories': self.ground_truth.get('max_categories'),\n",
    "            'missing_rate_value': self.ground_truth.get('missing_rate_value'),\n",
    "            'measurement_error_value': self.ground_truth.get('measurement_error_value'),\n",
    "            'missing_data_desc': self.ground_truth.get('missing_data_desc'),\n",
    "            'measurement_error_desc': self.ground_truth.get('measurement_error_desc'),\n",
    "        }\n",
    "        with open(config_filename, 'w') as f:\n",
    "            json.dump(config, f, indent=2, default=str)\n",
    "        # CHANGED: Replaced 'logger.success' with print\n",
    "        print(f\"\\u001b[92m✓ SUCCESS\\u001b[0m Config saved: {config_filename}\")\n",
    "\n",
    "    def generate_and_save_dataset(self, n_nodes: int, n_samples: int, output_dir: str = 'simulated_data', prefix: str = 'base', **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Generate a dataset and save the results.\n",
    "        \"\"\"\n",
    "        self.generate_dataset(n_nodes, n_samples, **kwargs)\n",
    "        self.save_simulation(output_dir, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ffd8e0-b5eb-441d-ba85-c8283d65d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_simulator = DataSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d325a900-7c56-4649-b65a-03343238a1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation: linear, gaussian, scale=1.0\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Data saved: simulated_data/20251116_184939_linear_gaussian_simple_nodes5_samples1000/linear_gaussian_simple_data.csv\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Graph saved: simulated_data/20251116_184939_linear_gaussian_simple_nodes5_samples1000/linear_gaussian_simple_graph.npy\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Config saved: simulated_data/20251116_184939_linear_gaussian_simple_nodes5_samples1000/linear_gaussian_simple_config.json\n"
     ]
    }
   ],
   "source": [
    "#1. Simple Linear Gaussian\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=5, \n",
    "    n_samples=1000, \n",
    "    function_type='linear', \n",
    "    noise_type='gaussian', \n",
    "    edge_probability=0.3,\n",
    "    prefix=\"linear_gaussian_simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97bb84b6-ceaf-4202-9164-09fd76ebc0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.148108</td>\n",
       "      <td>-0.270084</td>\n",
       "      <td>-0.217983</td>\n",
       "      <td>-1.499773</td>\n",
       "      <td>-1.019468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.787128</td>\n",
       "      <td>-0.014848</td>\n",
       "      <td>-1.122779</td>\n",
       "      <td>2.438146</td>\n",
       "      <td>0.584400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.184474</td>\n",
       "      <td>0.673437</td>\n",
       "      <td>-0.884064</td>\n",
       "      <td>0.749718</td>\n",
       "      <td>0.275555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407061</td>\n",
       "      <td>-0.905479</td>\n",
       "      <td>-1.961088</td>\n",
       "      <td>0.765910</td>\n",
       "      <td>0.317197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.564219</td>\n",
       "      <td>-0.179987</td>\n",
       "      <td>0.227717</td>\n",
       "      <td>-0.227827</td>\n",
       "      <td>1.648717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2        X3        X4        X5\n",
       "0 -0.148108 -0.270084 -0.217983 -1.499773 -1.019468\n",
       "1 -0.787128 -0.014848 -1.122779  2.438146  0.584400\n",
       "2  0.184474  0.673437 -0.884064  0.749718  0.275555\n",
       "3  0.407061 -0.905479 -1.961088  0.765910  0.317197\n",
       "4  2.564219 -0.179987  0.227717 -0.227827  1.648717"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'simulated_data/linear_gaussian_simple/linear_gaussian_simple_data.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04b990e-0151-4a4e-88a8-9a436d675bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded true DAG matrix:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "filepath = 'simulated_data/linear_gaussian_simple/linear_gaussian_simple_graph.npy'\n",
    "true_dag_matrix = np.load(filepath)\n",
    "\n",
    "print(\"Loaded true DAG matrix:\")\n",
    "print(true_dag_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214d6c2a-67fb-4795-8913-ee38b050a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation: linear, uniform, scale=1.0\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Data saved: simulated_data/20251116_193305_linear_uniform_noise_nodes5_samples1000/linear_uniform_noise_data.csv\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Graph saved: simulated_data/20251116_193305_linear_uniform_noise_nodes5_samples1000/linear_uniform_noise_graph.npy\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Config saved: simulated_data/20251116_193305_linear_uniform_noise_nodes5_samples1000/linear_uniform_noise_config.json\n"
     ]
    }
   ],
   "source": [
    "#2. Linear Non-Gaussian (Uniform Noise)\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=5, \n",
    "    n_samples=1000, \n",
    "    function_type='linear', \n",
    "    noise_type='uniform',  # changed noise type\n",
    "    edge_probability=0.3,\n",
    "    prefix=\"linear_uniform_noise\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5f8bcee-db9b-4ed9-8d15-e0e1e822db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation: mlp, gaussian, scale=1.0\n",
      "Non-linear function requires gaussian noise\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Data saved: simulated_data/20251116_193314_nonlinear_mlp_nodes10_samples2000/nonlinear_mlp_data.csv\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Graph saved: simulated_data/20251116_193314_nonlinear_mlp_nodes10_samples2000/nonlinear_mlp_graph.npy\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Config saved: simulated_data/20251116_193314_nonlinear_mlp_nodes10_samples2000/nonlinear_mlp_config.json\n"
     ]
    }
   ],
   "source": [
    "#3. Simple Non-linear (MLP)\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=10, \n",
    "    n_samples=2000, \n",
    "    function_type='mlp',  # non-linear function\n",
    "    noise_type='gaussian', # non-linear defaults to Gaussian noise\n",
    "    edge_probability=0.2,\n",
    "    prefix=\"nonlinear_mlp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "945a2768-4584-44fb-b7a6-9f34e3a89ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation: linear, gaussian, scale=1.0\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Data saved: simulated_data/20251116_193314_sparse_linear_graph_nodes15_samples1000/sparse_linear_graph_data.csv\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Graph saved: simulated_data/20251116_193314_sparse_linear_graph_nodes15_samples1000/sparse_linear_graph_graph.npy\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Config saved: simulated_data/20251116_193314_sparse_linear_graph_nodes15_samples1000/sparse_linear_graph_config.json\n"
     ]
    }
   ],
   "source": [
    "#4. Sparse Graph (low correlation) \n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=15, \n",
    "    n_samples=1000, \n",
    "    function_type='linear', \n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.05,  # low edge probability\n",
    "    prefix=\"sparse_linear_graph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f772eb11-3094-46bb-bfaf-88e20c9a8b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation: linear, gaussian, scale=1.0\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Data saved: simulated_data/20251116_193315_dense_scale_free_graph_nodes15_samples1000/dense_scale_free_graph_data.csv\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Graph saved: simulated_data/20251116_193315_dense_scale_free_graph_nodes15_samples1000/dense_scale_free_graph_graph.npy\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Config saved: simulated_data/20251116_193315_dense_scale_free_graph_nodes15_samples1000/dense_scale_free_graph_config.json\n"
     ]
    }
   ],
   "source": [
    "#5. Dense Graph (high correlation) \n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=15, \n",
    "    n_samples=1000, \n",
    "    function_type='linear', \n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.5,  # high edge probability\n",
    "    graph_type='SF',  # Scale-Free graph (creates hubs and not just random)\n",
    "    prefix=\"dense_scale_free_graph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99ce84be-c2cf-49bc-a314-af82d3a1916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation: linear, gaussian, scale=1.0\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Data saved: simulated_data/20251116_193315_mixed_data_discrete_nodes10_samples1500/mixed_data_discrete_data.csv\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Graph saved: simulated_data/20251116_193315_mixed_data_discrete_nodes10_samples1500/mixed_data_discrete_graph.npy\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Config saved: simulated_data/20251116_193315_mixed_data_discrete_nodes10_samples1500/mixed_data_discrete_config.json\n"
     ]
    }
   ],
   "source": [
    "#6. Mixed Data (continuous + discrete)\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=10, \n",
    "    n_samples=1500,\n",
    "    function_type='linear',\n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.3,\n",
    "    discrete_ratio=0.5,  # 50% of all nodes will be discrete\n",
    "    max_categories=4,\n",
    "    prefix=\"mixed_data_discrete\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce388334-6bee-45fc-bdee-2efc31c2b104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation: linear, gaussian, scale=1.0\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Data saved: simulated_data/20251116_193315_data_with_missing_nodes10_samples1000/data_with_missing_data.csv\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Graph saved: simulated_data/20251116_193315_data_with_missing_nodes10_samples1000/data_with_missing_graph.npy\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Config saved: simulated_data/20251116_193315_data_with_missing_nodes10_samples1000/data_with_missing_config.json\n"
     ]
    }
   ],
   "source": [
    "#7. Data with Missing Values \n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=10, \n",
    "    n_samples=1000,\n",
    "    function_type='linear',\n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.3,\n",
    "    add_missing_values=True,  # enabling missing values\n",
    "    missing_rate=0.1,  # 10% of data will be missing \n",
    "    prefix=\"data_with_missing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1580a8fc-209d-431c-bbdc-00804b0df68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation: linear, gaussian, scale=1.0\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Data saved: simulated_data/20251116_193316_data_with_error_nodes10_samples1000/data_with_error_data.csv\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Graph saved: simulated_data/20251116_193316_data_with_error_nodes10_samples1000/data_with_error_graph.npy\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Config saved: simulated_data/20251116_193316_data_with_error_nodes10_samples1000/data_with_error_config.json\n"
     ]
    }
   ],
   "source": [
    "#8. Data with Measurement Error\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=10, \n",
    "    n_samples=1000,\n",
    "    function_type='linear',\n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.3,\n",
    "    add_measurement_error=True,  # enable measurement error\n",
    "    error_rate=0.5,  # 50% of columns will have noise \n",
    "    error_std=0.4,   # sd of the added noise\n",
    "    prefix=\"data_with_error\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c72b774-10bd-46d8-af3a-089ce689c86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation: linear, gaussian, scale=1.0\n",
      "Domain affects 3 variables\n",
      "Base correlation matrix: (10, 10)\n",
      "Found 4 highly correlated pairs (base)\n",
      "Final correlation matrix: (10, 10)\n",
      "Found 5 highly correlated pairs (final)\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Data saved: simulated_data/20251116_193316_heterogeneous_multi_domain_nodes10_samples2000/heterogeneous_multi_domain_data.csv\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Graph saved: simulated_data/20251116_193316_heterogeneous_multi_domain_nodes10_samples2000/heterogeneous_multi_domain_graph.npy\n",
      "\u001b[92m✓ SUCCESS\u001b[0m Config saved: simulated_data/20251116_193316_heterogeneous_multi_domain_nodes10_samples2000/heterogeneous_multi_domain_config.json\n"
     ]
    }
   ],
   "source": [
    "#9. Heterogeneous Data (multiple domains)\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=10, \n",
    "    n_samples=2000,  # 500 samples per domain\n",
    "    function_type='linear',\n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.2,\n",
    "    n_domains=4,  # 4 distinct domains\n",
    "    prefix=\"heterogeneous_multi_domain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1bcb4-fc94-4116-81e1-2ae3d9b3223d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08171276-69d6-47ce-8015-aa0e5e3d6984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
