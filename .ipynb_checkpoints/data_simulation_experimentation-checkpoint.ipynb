{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f27d58d-a124-46b1-b10e-e342b17f869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "import igraph as ig\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Callable, Union\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "# CHANGE: The 'sklearn' import is needed for 'gp' and 'gp-add' function types\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def is_dag(W):\n",
    "    G = ig.Graph.Weighted_Adjacency(W.tolist())\n",
    "    return G.is_dag()\n",
    "\n",
    "def simulate_dag(d, s0, graph_type):\n",
    "    \"\"\"Simulate random DAG with some expected number of edges.\n",
    "    Args:\n",
    "        d (int): num of nodes\n",
    "        s0 (int): expected num of edges\n",
    "        graph_type (str): ER, SF, BP\n",
    "    Returns:\n",
    "        B (np.ndarray): [d, d] binary adj matrix of DAG\n",
    "    \"\"\"\n",
    "    def _random_permutation(M):\n",
    "        P = np.random.permutation(np.eye(M.shape[0]))\n",
    "        return P.T @ M @ P\n",
    "\n",
    "    def _random_acyclic_orientation(B_und):\n",
    "        return np.tril(_random_permutation(B_und), k=-1)\n",
    "\n",
    "    def _graph_to_adjmat(G):\n",
    "        return np.array(G.get_adjacency().data)\n",
    "\n",
    "    if graph_type == 'ER':\n",
    "        # Erdos-Renyi\n",
    "        G_und = ig.Graph.Erdos_Renyi(n=d, m=s0)\n",
    "        B_und = _graph_to_adjmat(G_und)\n",
    "        B = _random_acyclic_orientation(B_und)\n",
    "    elif graph_type == 'SF':\n",
    "        # Scale-free, Barabasi-Albert\n",
    "        G = ig.Graph.Barabasi(n=d, m=int(round(s0 / d)), directed=True)\n",
    "        B = _graph_to_adjmat(G)\n",
    "    elif graph_type == 'BP':\n",
    "        # Bipartite\n",
    "        top = int(0.2 * d)\n",
    "        G = ig.Graph.Random_Bipartite(top, d - top, m=s0, directed=True, neimode=ig.OUT)\n",
    "        B = _graph_to_adjmat(G)\n",
    "    else:\n",
    "        raise ValueError('unknown graph type')\n",
    "    B_perm = _random_permutation(B)\n",
    "    assert ig.Graph.Adjacency(B_perm.tolist()).is_dag()\n",
    "    return B_perm\n",
    "\n",
    "def simulate_parameter(B, w_ranges=((-2.0, -0.5), (0.5, 2.0))):\n",
    "    \"\"\"Simulate SEM parameters for a DAG.\n",
    "    Args:\n",
    "        B (np.ndarray): [d, d] binary adj matrix of DAG\n",
    "        w_ranges (tuple): disjoint weight ranges\n",
    "    Returns:\n",
    "        W (np.ndarray): [d, d] weighted adj matrix of DAG\n",
    "    \"\"\"\n",
    "    W = np.zeros(B.shape)\n",
    "    S = np.random.randint(len(w_ranges), size=B.shape)  # which range\n",
    "    for i, (low, high) in enumerate(w_ranges):\n",
    "        U = np.random.uniform(low=low, high=high, size=B.shape)\n",
    "        W += B * (S == i) * U\n",
    "    return W\n",
    "\n",
    "def simulate_linear_sem(W, n, sem_type, noise_scale=None, discrete_ratio=0.0, max_categories=10):\n",
    "    \"\"\"Simulate samples from linear SEM with specified type of noise and mixed continuous/discrete variables.\"\"\"\n",
    "    def _simulate_single_equation(X, w, scale, is_discrete=False, n_cats=None):\n",
    "        \"\"\"X: [n, num of parents], w: [num of parents], x: [n]\"\"\"\n",
    "        if is_discrete:\n",
    "            if X.shape[1] == 0:\n",
    "                logits = np.random.normal(scale=scale, size=(n, n_cats))\n",
    "            else:\n",
    "                logits = np.zeros((n, n_cats))\n",
    "                for k in range(n_cats):\n",
    "                    if sem_type == 'gaussian':\n",
    "                        logits[:, k] = X @ w\n",
    "                    elif sem_type == 'exponential':\n",
    "                        logits[:, k] = X @ w\n",
    "                    elif sem_type == 'gumbel':\n",
    "                        logits[:, k] = X @ w\n",
    "                    elif sem_type == 'uniform':\n",
    "                        logits[:, k] = X @ w\n",
    "                    else:\n",
    "                        raise ValueError('unsupported sem type for discrete variables')\n",
    "            probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "            return np.array([np.random.choice(n_cats, p=p) for p in probs]).astype(float)\n",
    "        else:\n",
    "            if sem_type == 'gaussian':\n",
    "                z = np.random.normal(scale=scale, size=n)\n",
    "                x = X @ w + z\n",
    "            elif sem_type == 'exponential':\n",
    "                z = np.random.exponential(scale=scale, size=n)\n",
    "                x = X @ w + z\n",
    "            elif sem_type == 'gumbel':\n",
    "                z = np.random.gumbel(scale=scale, size=n)\n",
    "                x = X @ w + z\n",
    "            elif sem_type == 'uniform':\n",
    "                z = np.random.uniform(low=-scale, high=scale, size=n)\n",
    "                x = X @ w + z\n",
    "            elif sem_type == 'logistic':\n",
    "                x = np.random.binomial(1, sigmoid(X @ w)) * 1.0\n",
    "            elif sem_type == 'poisson':\n",
    "                x = np.random.poisson(np.exp(X @ w)) * 1.0\n",
    "            else:\n",
    "                raise ValueError('unknown sem type')\n",
    "            return x\n",
    "\n",
    "    d = W.shape[0]\n",
    "    if noise_scale is None:\n",
    "        scale_vec = np.ones(d)\n",
    "    elif np.isscalar(noise_scale):\n",
    "        scale_vec = noise_scale * np.ones(d)\n",
    "    else:\n",
    "        if len(noise_scale) != d:\n",
    "            raise ValueError('noise scale must be a scalar or has length d')\n",
    "        scale_vec = noise_scale\n",
    "    if not is_dag(W):\n",
    "        raise ValueError('W must be a DAG')\n",
    "    if np.isinf(n):\n",
    "        if sem_type == 'gauss':\n",
    "            X = np.sqrt(d) * np.diag(scale_vec) @ np.linalg.inv(np.eye(d) - W)\n",
    "            return X\n",
    "        else:\n",
    "            raise ValueError('population risk not available')\n",
    "\n",
    "    n_discrete = int(d * discrete_ratio)\n",
    "    discrete_columns = np.random.choice(d, size=n_discrete, replace=False)\n",
    "\n",
    "    G = ig.Graph.Weighted_Adjacency(W.tolist())\n",
    "    ordered_vertices = G.topological_sorting()\n",
    "    assert len(ordered_vertices) == d\n",
    "    X = np.zeros([n, d])\n",
    "    for j in ordered_vertices:\n",
    "        parents = G.neighbors(j, mode=ig.IN)\n",
    "        is_discrete = j in discrete_columns\n",
    "        \n",
    "        # CHANGE: checker for max_categories\n",
    "        # would fail if max_categories was 2 (since low >= high)\n",
    "        if max_categories > 2:\n",
    "            n_categories = np.random.randint(2, max_categories)\n",
    "        else:\n",
    "            n_categories = 2\n",
    "            \n",
    "        X[:, j] = _simulate_single_equation(X[:, parents], W[parents, j], scale_vec[j], is_discrete, n_categories)\n",
    "    return X\n",
    "\n",
    "def simulate_nonlinear_sem(B, n, sem_type, noise_scale=None, discrete_ratio=0.3, max_categories=10):\n",
    "    \"\"\"Simulate samples from nonlinear SEM with mixed continuous and discrete variables.\"\"\"\n",
    "    def _simulate_single_equation(X, scale, is_discrete=False, n_cats=None):\n",
    "        pa_size = X.shape[1]\n",
    "        \n",
    "        if is_discrete:\n",
    "            if pa_size == 0:\n",
    "                logits = np.random.normal(scale=scale, size=(n, n_cats))\n",
    "            else:\n",
    "                if sem_type == 'mlp':\n",
    "                    hidden = 100\n",
    "                    logits = np.zeros((n, n_cats))\n",
    "                    for k in range(n_cats):\n",
    "                        W1 = np.random.uniform(low=0.5, high=2.0, size=[pa_size, hidden])\n",
    "                        W1[np.random.rand(*W1.shape) < 0.5] *= -1\n",
    "                        W2 = np.random.uniform(low=0.5, high=2.0, size=hidden)\n",
    "                        W2[np.random.rand(hidden) < 0.5] *= -1\n",
    "                        logits[:, k] = sigmoid(X @ W1) @ W2\n",
    "                elif sem_type == 'mim':\n",
    "                    logits = np.zeros((n, n_cats))\n",
    "                    for k in range(n_cats):\n",
    "                        w1 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                        w1[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                        w2 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                        w2[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                        w3 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                        w3[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                        logits[:, k] = np.tanh(X @ w1) + np.cos(X @ w2) + np.sin(X @ w3)\n",
    "                elif sem_type == 'gp' or sem_type == 'gp-add':\n",
    "                    # <<< CHANGE: This import was inside the function in your script.\n",
    "                    # This is fine, just confirming scikit-learn is a dependency.\n",
    "                    # from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "                    logits = np.zeros((n, n_cats))\n",
    "                    for k in range(n_cats):\n",
    "                        gp = GaussianProcessRegressor()\n",
    "                        if sem_type == 'gp':\n",
    "                            logits[:, k] = gp.sample_y(X, random_state=None).flatten()\n",
    "                        else:  # gp-add\n",
    "                            logits[:, k] = sum([gp.sample_y(X[:, i, None], random_state=None).flatten() \n",
    "                                                for i in range(X.shape[1])])\n",
    "                else:\n",
    "                    raise ValueError('unknown sem type')\n",
    "            \n",
    "            probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "            return np.array([np.random.choice(n_cats, p=p) for p in probs]).astype(float)\n",
    "            \n",
    "        else:\n",
    "            z = np.random.normal(scale=scale, size=n)\n",
    "            if pa_size == 0:\n",
    "                return z\n",
    "                \n",
    "            if sem_type == 'mlp':\n",
    "                hidden = 100\n",
    "                W1 = np.random.uniform(low=0.5, high=2.0, size=[pa_size, hidden])\n",
    "                W1[np.random.rand(*W1.shape) < 0.5] *= -1\n",
    "                W2 = np.random.uniform(low=0.5, high=2.0, size=hidden)\n",
    "                W2[np.random.rand(hidden) < 0.5] *= -1\n",
    "                x = sigmoid(X @ W1) @ W2 + z\n",
    "            elif sem_type == 'mim':\n",
    "                w1 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                w1[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                w2 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                w2[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                w3 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
    "                w3[np.random.rand(pa_size) < 0.5] *= -1\n",
    "                x = np.tanh(X @ w1) + np.cos(X @ w2) + np.sin(X @ w3) + z\n",
    "            elif sem_type == 'gp':\n",
    "                # from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "                gp = GaussianProcessRegressor()\n",
    "                x = gp.sample_y(X, random_state=None).flatten() + z\n",
    "            elif sem_type == 'gp-add':\n",
    "                # from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "                gp = GaussianProcessRegressor()\n",
    "                x = sum([gp.sample_y(X[:, i, None], random_state=None).flatten()\n",
    "                         for i in range(X.shape[1])]) + z\n",
    "            else:\n",
    "                raise ValueError('unknown sem type')\n",
    "            return x\n",
    "\n",
    "    d = B.shape[0]\n",
    "    scale_vec = noise_scale * np.ones(d) if noise_scale else np.ones(d)\n",
    "    X = np.zeros([n, d])\n",
    "    G = ig.Graph.Adjacency(B.tolist())\n",
    "    ordered_vertices = G.topological_sorting()\n",
    "    assert len(ordered_vertices) == d\n",
    "    \n",
    "    n_discrete = int(d * discrete_ratio)\n",
    "    discrete_columns = np.random.choice(d, size=n_discrete, replace=False)\n",
    "    \n",
    "    for j in ordered_vertices:\n",
    "        parents = G.neighbors(j, mode=ig.IN)\n",
    "        is_discrete = j in discrete_columns\n",
    "        \n",
    "        # CHANGE: checker\n",
    "        # would fail if max_categories was 2 (since low >= high)\n",
    "        if max_categories > 2:\n",
    "            n_categories = np.random.randint(2, max_categories)\n",
    "        else:\n",
    "            n_categories = 2\n",
    "\n",
    "        X[:, j] = _simulate_single_equation(X[:, parents], scale_vec[j], is_discrete, n_categories)\n",
    "    return X\n",
    "\n",
    "# --- DataSimulator Class from repo (modified) ---\n",
    "\n",
    "class DataSimulator:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.graph = None\n",
    "        self.ground_truth = {}\n",
    "        self.variable_names = None\n",
    "\n",
    "    def generate_graph(self, n_nodes: int, edge_probability: float = 0.3, variable_names: List[str] = None, graph_type: str = 'ER') -> None:\n",
    "        \"\"\"Generate a random directed acyclic graph (DAG) using specified method.\"\"\"\n",
    "        self.graph = simulate_dag(n_nodes, int(edge_probability * n_nodes * (n_nodes - 1) / 2), graph_type)\n",
    "        if variable_names and len(variable_names) == n_nodes:\n",
    "            self.variable_names = variable_names\n",
    "            self.graph_dict = {i: name for i, name in enumerate(variable_names)}\n",
    "        else:\n",
    "            self.variable_names = [f'X{i+1}' for i in range(n_nodes)]\n",
    "            self.graph_dict = {i: f'X{i+1}' for i in range(n_nodes)}\n",
    "        self.ground_truth['graph'] = self.graph_dict\n",
    "        self.ground_truth['edge_probability'] = edge_probability\n",
    "\n",
    "    def generate_single_domain_data(self, n_samples: int, noise_scale: float, noise_type: str, \n",
    "                                      function_type: Union[str, List[str], Dict[str, str]], \n",
    "                                      discrete_ratio: float = 0.0, max_categories: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Generate data for a single domain based on the graph structure.\"\"\"\n",
    "        # <<< CHANGE: Replaced 'logger.debug' with 'print'\n",
    "        print(f\"Data generation: {function_type}, {noise_type}, scale={noise_scale}\")\n",
    "        assert isinstance(function_type, str) and function_type in ['linear', 'mlp', 'mim', 'gp', 'gp-add']\n",
    "        assert noise_type in ['gaussian', 'exponential', 'gumbel', 'uniform', 'logistic', 'poisson']\n",
    "        if function_type != 'linear':\n",
    "            # CHANGE: replaced logger.debug with print\n",
    "            print(\"Non-linear function requires gaussian noise\")\n",
    "        assert isinstance(noise_scale, float) and noise_scale > 0\n",
    "        if function_type == 'linear':\n",
    "            W = simulate_parameter(self.graph)\n",
    "            data = simulate_linear_sem(W, n_samples, noise_type, noise_scale, discrete_ratio, max_categories)\n",
    "        else:\n",
    "            data = simulate_nonlinear_sem(self.graph, n_samples, function_type, noise_scale, discrete_ratio, max_categories)\n",
    "        \n",
    "        data_df = pd.DataFrame(data, columns=self.variable_names)\n",
    "        return data_df\n",
    "    \n",
    "    def generate_multi_domain_data(self, n_samples: int, noise_scale: float, noise_type: str, \n",
    "                                     function_type: Union[str, List[str], Dict[str, str]], \n",
    "                                     discrete_ratio: float = 0.0, max_categories: int = 5, edge_probability: float = 0.3) -> pd.DataFrame:\n",
    "        \"\"\"Generate data for multiple domains based on the graph structure.\"\"\"\n",
    "        # <<< CHANGE: Replaced 'logger.debug' with 'print'\n",
    "        print(f\"Data generation: {function_type}, {noise_type}, scale={noise_scale}\")\n",
    "        assert isinstance(function_type, str) and function_type in ['linear', 'mlp', 'mim', 'gp', 'gp-add']\n",
    "        assert noise_type in ['gaussian', 'exponential', 'gumbel', 'uniform', 'logistic', 'poisson']\n",
    "        if function_type != 'linear':\n",
    "            #  CHANGE: replaced logger.debug with print\n",
    "            print(\"Non-linear function requires gaussian noise\")\n",
    "        assert isinstance(noise_scale, float) and noise_scale > 0\n",
    "\n",
    "        if function_type == 'linear':\n",
    "            W = simulate_parameter(self.graph)\n",
    "            base_data = []\n",
    "            for i in range(self.n_domains):\n",
    "                base_data.append(simulate_linear_sem(W, n_samples, noise_type, noise_scale, discrete_ratio, max_categories))\n",
    "        else:\n",
    "            base_data = []\n",
    "            for i in range(self.n_domains):\n",
    "                base_data.append(simulate_nonlinear_sem(self.graph, n_samples, function_type, noise_scale, discrete_ratio, max_categories))\n",
    "        \n",
    "        n_nodes = self.graph.shape[0]\n",
    "        node_connections = np.sum(self.graph, axis=1) + np.sum(self.graph, axis=0)\n",
    "        less_connected_nodes = np.argsort(node_connections)\n",
    "        n_affected = max(3, int(edge_probability * n_nodes))\n",
    "        affected_nodes = less_connected_nodes[:n_affected]\n",
    "        \n",
    "        #  CHANGE: replaced logger.debug with print\n",
    "        print(f\"Domain affects {len(affected_nodes)} variables\")\n",
    "        \n",
    "        data = []\n",
    "        step = 10 / self.n_domains\n",
    "        \n",
    "        base_data_array = np.vstack([d for d in base_data])\n",
    "        base_correlation_matrix = np.corrcoef(base_data_array.T)\n",
    "        #  CHANGE: replaced logger.debug with print\n",
    "        print(f\"Base correlation matrix: {base_correlation_matrix.shape}\")\n",
    "        \n",
    "        n_vars = base_correlation_matrix.shape[0]\n",
    "        high_corr_threshold = 0.7\n",
    "        high_corr_pairs_base = []\n",
    "        \n",
    "        for i in range(n_vars):\n",
    "            for j in range(i+1, n_vars):\n",
    "                corr = base_correlation_matrix[i, j]\n",
    "                if abs(corr) > high_corr_threshold:\n",
    "                    high_corr_pairs_base.append((self.variable_names[i], self.variable_names[j], corr))\n",
    "        \n",
    "        if high_corr_pairs_base:\n",
    "            # CHANGE: replaced logger.debug with print\n",
    "            print(f\"Found {len(high_corr_pairs_base)} highly correlated pairs (base)\")\n",
    "        else:\n",
    "            #  CHANGE: replaced logger.debug with print\n",
    "            print(f\"No high correlations found (base)\")\n",
    "        \n",
    "        domain_effect_multiplier = 0.3\n",
    "        max_attempts = 5\n",
    "        attempts = 0\n",
    "        new_high_corr_pairs_found = False\n",
    "        \n",
    "        temp_data = [] # Initialize here\n",
    "        \n",
    "        while attempts < max_attempts and not new_high_corr_pairs_found:\n",
    "            temp_data = []\n",
    "            for i in range(self.n_domains):\n",
    "                domain_data = base_data[i].copy()\n",
    "                \n",
    "                if function_type == 'linear':\n",
    "                    domain_effect = (i + 1) * step * domain_effect_multiplier\n",
    "                    for node_idx in affected_nodes:\n",
    "                        domain_data[:, node_idx] += domain_effect\n",
    "                else:\n",
    "                    domain_effect = (i + 1) * step * domain_effect_multiplier\n",
    "                    for node_idx in affected_nodes:\n",
    "                        base_values = domain_data[:, node_idx]\n",
    "                        domain_data[:, node_idx] += domain_effect * np.sign(base_values) * (base_values ** 2)\n",
    "                \n",
    "                temp_data.extend(domain_data)\n",
    "            \n",
    "            temp_data_array = np.vstack(temp_data)\n",
    "            temp_correlation_matrix = np.corrcoef(temp_data_array.T)\n",
    "            \n",
    "            high_corr_pairs_temp = []\n",
    "            for i in range(n_vars):\n",
    "                for j in range(i+1, n_vars):\n",
    "                    corr = temp_correlation_matrix[i, j]\n",
    "                    if abs(corr) > high_corr_threshold:\n",
    "                        high_corr_pairs_temp.append((self.variable_names[i], self.variable_names[j], corr))\n",
    "            \n",
    "            if len(high_corr_pairs_temp) > len(high_corr_pairs_base):\n",
    "                new_high_corr_pairs_found = True\n",
    "                data = temp_data\n",
    "                correlation_matrix = temp_correlation_matrix\n",
    "                high_corr_pairs = high_corr_pairs_temp\n",
    "            else:\n",
    "                domain_effect_multiplier *= 1.5\n",
    "                attempts += 1\n",
    "        \n",
    "        if not new_high_corr_pairs_found:\n",
    "            data = temp_data\n",
    "            correlation_matrix = temp_correlation_matrix\n",
    "            high_corr_pairs = high_corr_pairs_temp\n",
    "        \n",
    "        #  CHANGE: replaced logger.debug with print\n",
    "        print(f\"Final correlation matrix: {correlation_matrix.shape}\")\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            #  CHANGE: replaced logger.debug with print\n",
    "            print(f\"Found {len(high_corr_pairs)} highly correlated pairs (final)\")\n",
    "        else:\n",
    "            #  CHANGE: replaced logger.debug with print\n",
    "            print(f\"No high correlations found (final)\")\n",
    "            \n",
    "        data_df = pd.DataFrame(data, columns=self.variable_names)\n",
    "        data_df['domain_index'] = np.repeat(range(1, 1 + self.n_domains), n_samples)\n",
    "        return data_df\n",
    "\n",
    "    def generate_data(self, n_samples: int, noise_scale: float = 1.0, \n",
    "                        noise_type: str = 'gaussian', \n",
    "                        function_type: Union[str, List[str], Dict[str, str]] = 'linear',\n",
    "                        n_domains: int = 1, variable_names: List[str] = None,\n",
    "                        discrete_ratio: float = 0.0, max_categories: int = 5, edge_probability: float = 0.3) -> None:\n",
    "        \"\"\"Generate heterogeneous data from multiple domains.\"\"\"\n",
    "        if self.graph is None:\n",
    "            raise ValueError(\"Generate graph first\")\n",
    "\n",
    "        domain_size = n_samples // n_domains\n",
    "        self.n_domains = n_domains\n",
    "\n",
    "        if n_domains == 1:\n",
    "            domain_df = self.generate_single_domain_data(domain_size, noise_scale, noise_type, function_type, \n",
    "                                                         discrete_ratio, max_categories)\n",
    "        else:\n",
    "            domain_df = self.generate_multi_domain_data(domain_size, noise_scale, noise_type, function_type, \n",
    "                                                        discrete_ratio, max_categories, edge_probability)\n",
    "        \n",
    "        self.data = domain_df\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        if variable_names is not None:\n",
    "            if n_domains == 1:\n",
    "                self.data.columns = variable_names\n",
    "            else:\n",
    "                self.data.columns = variable_names + ['domain_index']\n",
    "                \n",
    "        self.ground_truth['n_domains'] = n_domains\n",
    "        self.ground_truth['noise_type'] = noise_type\n",
    "        self.ground_truth['function_type'] = function_type\n",
    "        self.ground_truth['discrete_ratio'] = discrete_ratio\n",
    "        self.ground_truth['max_categories'] = max_categories\n",
    "\n",
    "    def add_measurement_error(self, error_std: float = 0.3, error_rate: float = 0.5) -> None:\n",
    "        \"\"\"Randomly sample a subset of columns to add gaussian measurement error.\"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Generate data first\")\n",
    "\n",
    "        available_cols = [col for col in self.data.columns if col != 'domain_index']\n",
    "        n_cols = int(error_rate * len(available_cols))\n",
    "        columns = np.random.choice(available_cols, size=n_cols, replace=False)\n",
    "\n",
    "        for col in columns:\n",
    "            if pd.api.types.is_numeric_dtype(self.data[col]):\n",
    "                self.data[col] += np.random.randn(len(self.data)) * error_std\n",
    "        \n",
    "        self.ground_truth['measurement_error'] = {col: error_std for col in columns}\n",
    "        self.ground_truth['measurement_error_value'] = error_rate\n",
    "        self.ground_truth['measurement_error_desc'] = f\"The measurement error is Gaussian with a standard deviation of {error_std} on {error_rate} of the columns (some are selected, some are not, except domain_index), and the measurement error is added to the original data.\"\n",
    "\n",
    "    def add_missing_values(self, missing_rate: float = 0.1) -> None:\n",
    "        \"\"\"Introduce missing values to the whole dataframe with a specified missing rate.\"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Generate data first\")\n",
    "\n",
    "        columns = [col for col in self.data.columns if col != 'domain_index']\n",
    "        \n",
    "        mask = np.random.random(size=self.data[columns].shape) < missing_rate\n",
    "        \n",
    "        self.data[columns] = np.where(mask, np.nan, self.data[columns])\n",
    "        \n",
    "        affected_columns = [col for col in columns if mask[:, columns.index(col)].any()]\n",
    "        self.ground_truth['missing_rate'] = {col: missing_rate for col in affected_columns}\n",
    "        self.ground_truth['missing_rate_value'] = missing_rate\n",
    "        self.ground_truth['missing_data_desc'] = f\"The missing values are randomly sampled with a missing rate of {missing_rate} on all column data (except domain_index), and the missing values are replaced with NaN.\"\n",
    "\n",
    "    def generate_dataset(self, n_nodes: int, n_samples: int, edge_probability: float = 0.3,\n",
    "                         noise_scale: float = 1.0, noise_type: str = 'gaussian',\n",
    "                         function_type: Union[str, List[str], Dict[str, str]] = 'linear', discrete_ratio: float = 0.0, max_categories: int = 5,\n",
    "                         add_measurement_error: bool = False, add_missing_values: bool = False, n_domains: int = 1, \n",
    "                         error_std: float = 0.3, error_rate: float = 0.5, missing_rate: float = 0.1,\n",
    "                         variable_names: List[str] = None, graph_type: str = 'ER') -> Tuple[Dict[int, str], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Generate a complete heterogeneous dataset with various characteristics.\n",
    "        \"\"\"\n",
    "        self.generate_graph(n_nodes, edge_probability, variable_names, graph_type)\n",
    "        self.generate_data(n_samples, noise_scale, noise_type, function_type, n_domains, variable_names, discrete_ratio, max_categories, edge_probability)\n",
    "                \n",
    "        if add_measurement_error:\n",
    "            self.add_measurement_error(error_std=error_std, error_rate=error_rate)\n",
    "        else:\n",
    "            self.ground_truth['measurement_error'] = None\n",
    "            self.ground_truth['measurement_error_value'] = None\n",
    "            self.ground_truth['measurement_error_desc'] = None\n",
    "        \n",
    "        if add_missing_values:\n",
    "            self.add_missing_values(missing_rate=missing_rate)\n",
    "        else:\n",
    "            self.ground_truth['missing_rate'] = None\n",
    "            self.ground_truth['missing_rate_value'] = None\n",
    "            self.ground_truth['missing_data_desc'] = None\n",
    "        \n",
    "        return self.graph.T, self.data\n",
    "\n",
    "    def save_simulation(self, output_dir: str = 'simulated_data', prefix: str = 'base') -> None:\n",
    "        \"\"\"\n",
    "        Save the simulated data, graph structure, and simulation settings.\n",
    "        \"\"\"\n",
    "        if self.data is None or self.graph is None:\n",
    "            raise ValueError(\"No data or graph to save. Generate dataset first.\")\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        n_nodes = len(self.graph)\n",
    "        n_samples = len(self.data)\n",
    "        folder_name = f\"{timestamp}_{prefix}_nodes{n_nodes}_samples{n_samples}\"\n",
    "        save_dir = os.path.join(output_dir, folder_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        data_filename = os.path.join(save_dir, f'{prefix}_data.csv')\n",
    "        data_to_save = self.data.copy()\n",
    "        if self.ground_truth.get('n_domains', 1) == 1 and 'domain_index' in data_to_save.columns:\n",
    "            data_to_save = data_to_save.drop('domain_index', axis=1)\n",
    "        data_to_save.to_csv(data_filename, index=False)\n",
    "        # <<< CHANGE: Replaced 'logger.success' with 'print', keeping your notebook's success format.\n",
    "        print(f\"\\u001b[92m✓ SUCCESS\\u001b[0m Data saved: {data_filename}\")\n",
    "        \n",
    "        graph_filename = os.path.join(save_dir, f'{prefix}_graph.npy')\n",
    "        np.save(graph_filename, self.graph.T)\n",
    "        # <<< CHANGE: Replaced 'logger.success' with 'print'.\n",
    "        print(f\"\\u001b[92m✓ SUCCESS\\u001b[0m Graph saved: {graph_filename}\")\n",
    "        \n",
    "        config_filename = os.path.join(save_dir, f'{prefix}_config.json')\n",
    "        config = {\n",
    "            'n_nodes': n_nodes,\n",
    "            'n_samples': n_samples,\n",
    "            'n_domains': self.ground_truth.get('n_domains'),\n",
    "            'noise_type': self.ground_truth.get('noise_type'),\n",
    "            'function_type': self.ground_truth.get('function_type'),\n",
    "            'node_functions': self.ground_truth.get('node_functions'),\n",
    "            'categorical': self.ground_truth.get('categorical'),\n",
    "            'measurement_error': self.ground_truth.get('measurement_error'),\n",
    "            'selection_bias': self.ground_truth.get('selection_bias'),\n",
    "            'confounding': self.ground_truth.get('confounding'),\n",
    "            'missing_rate': self.ground_truth.get('missing_rate'),\n",
    "            'edge_probability': self.ground_truth.get('edge_probability'),\n",
    "            'discrete_ratio': self.ground_truth.get('discrete_ratio'),\n",
    "            'max_categories': self.ground_truth.get('max_categories'),\n",
    "            'missing_rate_value': self.ground_truth.get('missing_rate_value'),\n",
    "            'measurement_error_value': self.ground_truth.get('measurement_error_value'),\n",
    "            'missing_data_desc': self.ground_truth.get('missing_data_desc'),\n",
    "            'measurement_error_desc': self.ground_truth.get('measurement_error_desc'),\n",
    "        }\n",
    "        with open(config_filename, 'w') as f:\n",
    "            json.dump(config, f, indent=2, default=str)\n",
    "        # <<< CHANGE: Replaced 'logger.success' with 'print'.\n",
    "        print(f\"\\u001b[92m✓ SUCCESS\\u001b[0m Config saved: {config_filename}\")\n",
    "\n",
    "    def generate_and_save_dataset(self, n_nodes: int, n_samples: int, output_dir: str = 'simulated_data', prefix: str = 'base', **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Generate a dataset and save the results.\n",
    "        \"\"\"\n",
    "        self.generate_dataset(n_nodes, n_samples, **kwargs)\n",
    "        self.save_simulation(output_dir, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ffd8e0-b5eb-441d-ba85-c8283d65d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_simulator = DataSimulator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325a900-7c56-4649-b65a-03343238a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Simple Linear Gaussian\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=5, \n",
    "    n_samples=1000, \n",
    "    function_type='linear', \n",
    "    noise_type='gaussian', \n",
    "    edge_probability=0.3,\n",
    "    prefix=\"linear_gaussian_simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d6c2a-67fb-4795-8913-ee38b050a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Linear Non-Gaussian (Uniform Noise)\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=5, \n",
    "    n_samples=1000, \n",
    "    function_type='linear', \n",
    "    noise_type='uniform',  # changed noise type\n",
    "    edge_probability=0.3,\n",
    "    prefix=\"linear_uniform_noise\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8bcee-db9b-4ed9-8d15-e0e1e822db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Simple Non-linear (MLP)\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=10, \n",
    "    n_samples=2000, \n",
    "    function_type='mlp',  # non-linear function\n",
    "    noise_type='gaussian', # non-linear defaults to Gaussian noise\n",
    "    edge_probability=0.2,\n",
    "    prefix=\"nonlinear_mlp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a2768-4584-44fb-b7a6-9f34e3a89ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Sparse Graph (low correlation) \n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=15, \n",
    "    n_samples=1000, \n",
    "    function_type='linear', \n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.05,  # low edge probability\n",
    "    prefix=\"sparse_linear_graph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772eb11-3094-46bb-bfaf-88e20c9a8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Dense Graph (high correlation) \n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=15, \n",
    "    n_samples=1000, \n",
    "    function_type='linear', \n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.5,  # high edge probability\n",
    "    graph_type='SF',  # Scale-Free graph (creates hubs and not just random)\n",
    "    prefix=\"dense_scale_free_graph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce84be-c2cf-49bc-a314-af82d3a1916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Mixed Data (continuous + discrete)\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=10, \n",
    "    n_samples=1500,\n",
    "    function_type='linear',\n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.3,\n",
    "    discrete_ratio=0.5,  # 50% of all nodes will be discrete\n",
    "    max_categories=4,\n",
    "    prefix=\"mixed_data_discrete\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce388334-6bee-45fc-bdee-2efc31c2b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Data with Missing Values \n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=10, \n",
    "    n_samples=1000,\n",
    "    function_type='linear',\n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.3,\n",
    "    add_missing_values=True,  # enabling missing values\n",
    "    missing_rate=0.1,  # 10% of data will be missing \n",
    "    prefix=\"data_with_missing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580a8fc-209d-431c-bbdc-00804b0df68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Data with Measurement Error\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=10, \n",
    "    n_samples=1000,\n",
    "    function_type='linear',\n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.3,\n",
    "    add_measurement_error=True,  # enable measurement error\n",
    "    error_rate=0.5,  # 50% of columns will have noise \n",
    "    error_std=0.4,   # sd of the added noise\n",
    "    prefix=\"data_with_error\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72b774-10bd-46d8-af3a-089ce689c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Heterogeneous Data (multiple domains)\n",
    "base_simulator.generate_and_save_dataset(\n",
    "    n_nodes=10, \n",
    "    n_samples=2000,  # 500 samples per domain\n",
    "    function_type='linear',\n",
    "    noise_type='gaussian',\n",
    "    edge_probability=0.2,\n",
    "    n_domains=4,  # 4 distinct domains\n",
    "    prefix=\"heterogeneous_multi_domain\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
